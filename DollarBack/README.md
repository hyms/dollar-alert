DollarBack
==========

ğŸ“ Description
--------------

The core engine of the DollarAlert ecosystem. This service is responsible for data acquisition through dynamic web scraping, storage in Supabase, and dispatching multi-channel notifications. It is built using **Clean Architecture** principles to ensure maintainability and scalability.

ğŸ¯ Core Responsibilities
------------------------

-   **Dynamic Web Scraping**: Fetches exchange rates from multiple sources defined in the database configuration.

-   **Multi-Channel Notifications**: Integrated with Telegram and Web Push notifications.

-   **Data Persistence**: Manages historical and real-time data using Supabase (PostgreSQL).

-   **Admin API**: Secure endpoints for the administrative portal to manage scrapers and system settings.

ğŸš€ Tech Stack & Libraries
-------------------------

-   **Runtime**: Node.js (v20+)

-   **Framework**: [Fastify](https://www.fastify.io/ "null") (Plugin-based architecture)

-   **Language**: TypeScript

-   **Database Client**: `@supabase/supabase-js`

-   **Bot Engine**: `telegraf` (Telegram API)

-   **Push Engine**: `web-push`

-   **Scraping**: `cheerio` & `puppeteer-core`

-   **Task Scheduling**: `@fastify/schedule`

ğŸ—ï¸ Clean Architecture Structure
--------------------------------

```
DollarBack/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ domain/               # Enterprise Logic
â”‚   â”‚   â”œâ”€â”€ entities/         # Currency, Rate, Subscriber, Config
â”‚   â”‚   â””â”€â”€ repositories/     # Interface definitions
â”‚   â”œâ”€â”€ application/          # Business Rules
â”‚   â”‚   â”œâ”€â”€ use-cases/        # ScrapingProcess, NotifyUsers
â”‚   â”‚   â””â”€â”€ services/         # ScraperEngine, NotificationManager
â”‚   â”œâ”€â”€ infrastructure/       # External Tools
â”‚   â”‚   â”œâ”€â”€ database/         # Supabase repositories
â”‚   â”‚   â”œâ”€â”€ external-api/     # Scrapers logic
â”‚   â”‚   â”œâ”€â”€ delivery/         # Fastify Routes
â”‚   â”‚   â””â”€â”€ messaging/        # Telegram & WebPush
â”‚   â””â”€â”€ main/                 # Entry point & Wiring
â”œâ”€â”€ migrations/               # SQL Migration files
â””â”€â”€ .env.example

```

ğŸ—„ï¸ Database Schema (Migrations)
--------------------------------

These are the base tables required for the system to function. You can find the full SQL in `DollarUi/database/schema.sql`.

### 1\. `admin_configs`

Stores system settings, admin credentials, and dynamic scraping targets.

-   **Fields**: `admin_username`, `admin_password_hash`, `scraping_sources` (JSONB), `maintenance_mode`.

### 2\. `exchange_rates_history`

Stores the time-series data for historical analysis.

-   **Fields**: `currency_code`, `official_buy`, `official_sell`, `parallel_buy`, `parallel_sell`, `captured_at`.

### 3\. `notification_subscribers`

Registry for all devices and accounts receiving alerts.

-   **Fields**: `user_identifier` (Telegram ID / UUID), `platform` (telegram/web_push), `push_subscription_data` (JSONB).

ğŸ”„ Database Migrations (Supabase)
---------------------------------

1.  **Local Development**: Create a new `.sql` file in `/migrations`.

2.  **Deployment**: Use the Supabase CLI:

    ```
    supabase db push --linked

    ```

ğŸ•·ï¸ Dynamic Web Scraping
------------------------

The `ScraperEngine` fetches targets from the `admin_configs` table. Each source in the `scraping_sources` JSONB array must define:

-   `url`: Target website.

-   `selector`: CSS selector for the rate.

-   `currency`: Currency code (default: USD).

-   `frequency`: Cron-style or preset interval.

ğŸ› ï¸ How to Run
--------------

### Development (Docker)

```
# From project root
docker-compose up backend

```

ğŸ”Œ Environment Variables
------------------------

```
PORT=3000
SUPABASE_URL=your_project_url
SUPABASE_KEY=your_service_role_key
TELEGRAM_TOKEN=your_bot_token
VAPID_PUBLIC_KEY=for_web_push
VAPID_PRIVATE_KEY=for_web_push

```

ğŸ“ License
----------

This project is licensed under the ISC License.
